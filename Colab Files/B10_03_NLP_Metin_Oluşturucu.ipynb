{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "B10_03_NLP_Metin_Oluşturucu.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXL5LqKPyFTu"
      },
      "source": [
        "# Metin Oluşturu Model İnşa Etme"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sEm5JxdyLne"
      },
      "source": [
        "Halihazırda öğrenmiş olduğunuz tekniklerin çoğunu kullanarak, belirli bir ana kelimeyi takip eden bir sonraki kelimeyi tahmin ederek yeni metin oluşturmak artık mümkün. Bu yöntemi uygulamak için [Kaggle Şarkı Sözleri Veri Kümesini]((https://www.kaggle.com/mousehead/songlyrics)) kullanacağız."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S42_4o3NyhGx"
      },
      "source": [
        "## TensorFlow ve Gereli Fonksiyonların İçeri Aktarılması"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8iBWTXwyAvt"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Verilerin işlenmesi için diğer içeri aktarmalar\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZlEJfj5yszA"
      },
      "source": [
        "## Veri Setinin Alınması\n",
        "\n",
        "Yukarıda belirttiğimiz gibi bu colab dosyasında model geliştirmek için şarkı sözleri veri setini kullanacağız."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBDcanucyq7G",
        "outputId": "50626b75-836e-4b79-d02d-a0ff0169eacd"
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-28 20:25:55--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.9.206, 2607:f8b0:4004:806::200e\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.9.206|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/f58oiassacf7bh3bmkqa7c7dtaf96pt3/1627503900000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-07-28 20:25:57--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/f58oiassacf7bh3bmkqa7c7dtaf96pt3/1627503900000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 172.217.164.129, 2607:f8b0:4004:814::2001\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|172.217.164.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv       [   <=>              ]  69.08M   122MB/s    in 0.6s    \n",
            "\n",
            "2021-07-28 20:25:58 (122 MB/s) - ‘/tmp/songdata.csv’ saved [72436445]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDd2_-54y80v"
      },
      "source": [
        "Önce veri setinden sadece 10 şarkıya bakalım ve işlerin nasıl performans gösterdiğini görelim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDY4PGQBy_Vx"
      },
      "source": [
        "## Ön İşleme Yapılması\n",
        "\n",
        "Noktalama işaretlerinden kurtulmak ve her şeyi küçük harf yapmak için bazı temel önişlemeler yapalım. Daha sonra sözleri satırlara bölüp şarkı sözlerini belirteceğiz.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EvxYI59y46V"
      },
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Tokenize ediciyi yerleştirin\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Diğer tüm noktalama işaretlerini kaldır\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # küçük harf yap\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Satıra göre bölmek için uzun bir dize yapın\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Sondaki boşlukları kaldırın\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Boş satırları kaldırın\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bGhGZVCzWA-",
        "outputId": "87762ff9-8287-43c7-cf93-59a310dd0fa8"
      },
      "source": [
        "# Veri kümesini csv'den okuyun - şimdilik sadece ilk 10 şarkı\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Şarkı sözlerini içeren 'metin' sütununu kullanarak derlemi oluşturun\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize edin\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT5SF-OrzhlH"
      },
      "source": [
        "## Dizilerin ve Etiketlerin Oluşturulması\n",
        "\n",
        "Ön işlemeden sonra, sıralar ve etiketler oluşturmamız gerekiyor. Dizileri kendileri oluşturmak (`texts_to_sequences` ile öncekine benzer) ancak [N-Grams]((https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9)) kullanımını da içerir; etiketlerin oluşturulması artık bu dizileri kullanacak ve tüm potansiyel çıktı sözcükleri üzerinde `one-hot` kodlamayı kullanacak.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppm-pD37ze0V"
      },
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Eşit giriş uzunluğu için dolgu dizileri\n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# \"Giriş\" dizisi ve \"çıkış\" tahmin edilen kelime arasında dizileri bölme\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# Etiketlere one-hot kodlama\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyiVBHw70CWo",
        "outputId": "ab338e16-f254-4af5-ee70-d742bb65e65f"
      },
      "source": [
        "# Bazı verilerimizin nasıl saklandığını kontrol edin\n",
        "# Tokenizer, kelime başına yalnızca tek bir dizine sahiptir\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Giriş dizileri birden çok dizine sahip olacak\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# Ve one-hot etiket, tokenize edilmiş kelimelerin tam olarak yayılması kadar uzun olacaktır\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAQrVZEd0T6p"
      },
      "source": [
        "## Metin Oluşturucu Modelin Eğitilmesi\n",
        "\n",
        "Metin oluşturma modelimizi eğitmek için bir RNN oluşturmak, daha önce oluşturduğunuz duygu modellerine çok benzer olacaktır. Gerekli olan tek gerçek değişiklik, kayıp işlevi olarak İkili Çapraz Entropi (Binary Cross Entropy) yerine Kategorik'i (Categorical) kullandığınızdan emin olmaktır - daha önce duygu yalnızca 0 veya 1 olduğundan Binary'yi kullanabilirdik, ancak şimdi yüzlerce kategori var.\n",
        "\n",
        "Metin oluşturmanın yakınsaması duygu analizine göre biraz daha uzun sürebileceğinden ve henüz o kadar çok veriyle çalışmıyoruz, çünkü bu noktadan sonra, öncekinden daha fazla dönem kullanmayı düşünmeliyiz. Veri kümesinin yalnızca bir kısmını kullandığımız için burada 200 çağa ayarlıyoruz ve eğitim bu kadar çok çağdan biraz daha geride kalacak.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lANrtATZ0SIw",
        "outputId": "28851e2f-8806-42a2-a155-1aba91aaff27"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 10s 8ms/step - loss: 5.9906 - accuracy: 0.0283\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 5.4349 - accuracy: 0.0358\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 5.3726 - accuracy: 0.0399\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 5.3119 - accuracy: 0.0399\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 5.2440 - accuracy: 0.0318\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 5.1822 - accuracy: 0.0394\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 5.1251 - accuracy: 0.0394\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 5.0734 - accuracy: 0.0434\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 5.0173 - accuracy: 0.0439\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 4.9573 - accuracy: 0.0530\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 4.8890 - accuracy: 0.0580\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 4.8078 - accuracy: 0.0605\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 4.7148 - accuracy: 0.0701\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 4.6227 - accuracy: 0.0716\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 4.5219 - accuracy: 0.0807\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 4.4253 - accuracy: 0.1019\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 4.3237 - accuracy: 0.1216\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 4.2215 - accuracy: 0.1317\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 4.1291 - accuracy: 0.1630\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 4.0381 - accuracy: 0.1781\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 3.9492 - accuracy: 0.1932\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 3.8542 - accuracy: 0.2170\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.7733 - accuracy: 0.2381\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.6743 - accuracy: 0.2477\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.6041 - accuracy: 0.2679\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 3.5185 - accuracy: 0.2962\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 3.4442 - accuracy: 0.3078\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.3623 - accuracy: 0.3214\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.2955 - accuracy: 0.3446\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.2328 - accuracy: 0.3517\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 3.1538 - accuracy: 0.3607\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.0937 - accuracy: 0.3789\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.0360 - accuracy: 0.3799\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 2.9628 - accuracy: 0.4077\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.9024 - accuracy: 0.4157\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 2.8414 - accuracy: 0.4238\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.7829 - accuracy: 0.4495\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.7383 - accuracy: 0.4470\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 2.6794 - accuracy: 0.4617\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 2.6404 - accuracy: 0.4677\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 2.5736 - accuracy: 0.4823\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 2.5161 - accuracy: 0.4929\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.4664 - accuracy: 0.4960\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 2.4170 - accuracy: 0.5141\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 2.3692 - accuracy: 0.5252\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 2.3226 - accuracy: 0.5212\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.2850 - accuracy: 0.5288\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 2.2493 - accuracy: 0.5388\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.2061 - accuracy: 0.5540\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 2.1509 - accuracy: 0.5651\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.1149 - accuracy: 0.5686\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.0758 - accuracy: 0.5762\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.0411 - accuracy: 0.5848\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.0058 - accuracy: 0.5888\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.9791 - accuracy: 0.5923\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.9556 - accuracy: 0.5999\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.9143 - accuracy: 0.6075\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.8618 - accuracy: 0.6171\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.8247 - accuracy: 0.6297\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.7853 - accuracy: 0.6387\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 1.7718 - accuracy: 0.6362\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.7777 - accuracy: 0.6322\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 1.7185 - accuracy: 0.6579\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.6978 - accuracy: 0.6529\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.6684 - accuracy: 0.6690\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.6413 - accuracy: 0.6680\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.6023 - accuracy: 0.6746\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.6350 - accuracy: 0.6579\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.5947 - accuracy: 0.6791\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.5358 - accuracy: 0.6942\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.5142 - accuracy: 0.6922\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.4788 - accuracy: 0.6973\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.4571 - accuracy: 0.7069\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.4263 - accuracy: 0.7164\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.3950 - accuracy: 0.7190\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.3688 - accuracy: 0.7265\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.3423 - accuracy: 0.7366\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.3246 - accuracy: 0.7376\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.3019 - accuracy: 0.7432\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.2776 - accuracy: 0.7508\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.2614 - accuracy: 0.7528\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.2477 - accuracy: 0.7588\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.2517 - accuracy: 0.7477\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.2347 - accuracy: 0.7513\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.2000 - accuracy: 0.7598\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.1885 - accuracy: 0.7654\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.1632 - accuracy: 0.7634\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.1488 - accuracy: 0.7689\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.1187 - accuracy: 0.7830\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.1058 - accuracy: 0.7841\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.0835 - accuracy: 0.7901\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.0706 - accuracy: 0.7931\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.0476 - accuracy: 0.8032\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.0263 - accuracy: 0.8032\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.0245 - accuracy: 0.8047\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.0198 - accuracy: 0.8007\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9968 - accuracy: 0.8047\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9825 - accuracy: 0.8113\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9625 - accuracy: 0.8138\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9451 - accuracy: 0.8163\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9404 - accuracy: 0.8143\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.0024 - accuracy: 0.7962\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9617 - accuracy: 0.8042\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9299 - accuracy: 0.8153\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9044 - accuracy: 0.8244\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9374 - accuracy: 0.8158\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.9105 - accuracy: 0.8133\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.8805 - accuracy: 0.8229\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.8584 - accuracy: 0.8300\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.8611 - accuracy: 0.8249\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.8486 - accuracy: 0.8330\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.8293 - accuracy: 0.8310\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.8160 - accuracy: 0.8365\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7973 - accuracy: 0.8426\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7864 - accuracy: 0.8421\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7745 - accuracy: 0.8476\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7664 - accuracy: 0.8481\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7609 - accuracy: 0.8436\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7453 - accuracy: 0.8512\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7369 - accuracy: 0.8491\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7245 - accuracy: 0.8537\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7129 - accuracy: 0.8527\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7096 - accuracy: 0.8552\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7169 - accuracy: 0.8557\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7799 - accuracy: 0.8330\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7378 - accuracy: 0.8461\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.6971 - accuracy: 0.8557\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.6962 - accuracy: 0.8572\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.6667 - accuracy: 0.8597\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.6526 - accuracy: 0.8623\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.6457 - accuracy: 0.8623\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.6402 - accuracy: 0.8633\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.6373 - accuracy: 0.8643\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.6262 - accuracy: 0.8678\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.6176 - accuracy: 0.8663\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.6178 - accuracy: 0.8708\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.6046 - accuracy: 0.8703\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5978 - accuracy: 0.8718\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5933 - accuracy: 0.8729\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5855 - accuracy: 0.8754\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5825 - accuracy: 0.8739\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.5819 - accuracy: 0.8718\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5750 - accuracy: 0.8744\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5625 - accuracy: 0.8799\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5521 - accuracy: 0.8794\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5474 - accuracy: 0.8829\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5440 - accuracy: 0.8819\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5365 - accuracy: 0.8804\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5304 - accuracy: 0.8845\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.5349 - accuracy: 0.8814\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5273 - accuracy: 0.8779\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5148 - accuracy: 0.8819\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.5144 - accuracy: 0.8829\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5090 - accuracy: 0.8875\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5059 - accuracy: 0.8835\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5037 - accuracy: 0.8855\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5073 - accuracy: 0.8789\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5056 - accuracy: 0.8794\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4987 - accuracy: 0.8819\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4955 - accuracy: 0.8814\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4892 - accuracy: 0.8829\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4866 - accuracy: 0.8814\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4805 - accuracy: 0.8845\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4680 - accuracy: 0.8885\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4578 - accuracy: 0.8905\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4526 - accuracy: 0.8905\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4480 - accuracy: 0.8870\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4421 - accuracy: 0.8905\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4381 - accuracy: 0.8946\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4381 - accuracy: 0.8890\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4358 - accuracy: 0.8915\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4388 - accuracy: 0.8900\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4278 - accuracy: 0.8925\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4204 - accuracy: 0.8956\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4228 - accuracy: 0.8920\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4292 - accuracy: 0.8930\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4232 - accuracy: 0.8915\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4169 - accuracy: 0.8920\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4081 - accuracy: 0.8920\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4144 - accuracy: 0.8920\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4153 - accuracy: 0.8895\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4065 - accuracy: 0.8976\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4168 - accuracy: 0.8900\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4178 - accuracy: 0.8935\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4119 - accuracy: 0.8940\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4021 - accuracy: 0.8910\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3993 - accuracy: 0.8946\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3942 - accuracy: 0.8940\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3799 - accuracy: 0.8986\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3768 - accuracy: 0.9016\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3710 - accuracy: 0.8996\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3669 - accuracy: 0.8946\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3632 - accuracy: 0.9006\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3596 - accuracy: 0.8996\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3550 - accuracy: 0.9026\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3523 - accuracy: 0.9006\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3510 - accuracy: 0.9026\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3487 - accuracy: 0.9011\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.3451 - accuracy: 0.9026\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3421 - accuracy: 0.9062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szTfj7iX0sRs"
      },
      "source": [
        "## Eğitim Grafiğinin Oluşturulması \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "9FPqq5TY0q_-",
        "outputId": "209492cd-b448-4c79-f436-20df75156c08"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Yineleme (Epochs)\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1ZxEGFz034S"
      },
      "source": [
        "## Yeni Şarkı Sözleri Oluşturalım\n",
        "\n",
        "Sonunda, eğitilmiş modelden yeni sözler üretmenin ve ne elde ettiğimizi görmenin zamanı geldi. Bunu yapmak için, modelin başlaması için bir miktar \"tohum metni\" (seed text) veya bir giriş sırası sağlayacağız. Ayrıca bir çıktı dizisinin ne kadar uzun olmasını istediğimize de karar vereceğiz - girdi artı önceki çıktı sürekli olarak yeni bir çıktı kelimesi için besleneceğinden (en azından maksimum dizi uzunluğumuza kadar) bu aslında sonsuz olabilir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKtJDRSE02xl",
        "outputId": "181caf76-9fbf-482f-b3a0-0ef61849a4b3"
      },
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "im feeling chills me to me every little touch touch lousy night night night night night night dont surrender surrender dont would surrender sky and with again and you gently more you leave me girl life us would feet feet night life sun have fellow be bone learn figure tiny leaving feeling cry words more of life of yourself once more more you leave more you leave talk feel talk night night night question of girl surrender surrender surrender surrender surrender surrender surrender surrender surrender surrender surrender would andante behind dont surrender surrender surrender surrender would front feet night night night night night\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}